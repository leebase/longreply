---
layout: default
title: "The Modern SQL & Data Management Bootcamp: From Zero to Proficient with SQLite, DBeaver, and AI"
permalink: /sqltut-gemini/
---
The Modern SQL & Data Management Bootcamp: From Zero to Proficient with SQLite, DBeaver, and AIPart I: Foundation and SetupChapter 1: Your Modern Data Toolkit1.1 Introduction to the Course PhilosophyThis program is designed for the pragmatic upskiller—the developer, analyst, or system administrator who is already technically proficient but new to the world of data management. The curriculum is built on a philosophy of practical, hands-on learning that translates directly to real-world scenarios. It eschews theoretical minutiae in favor of building immediately applicable skills. The core objective is to teach not just the "how" of SQL syntax but the fundamental "why" behind data management principles. Upon completion, a learner will not only be able to write queries but will also have begun to think like a data professional, capable of designing, managing, and interrogating data systems effectively.The structure of this course is intentional. It begins with a solid foundation in database concepts, moves swiftly into the core mechanics of the SQL language, and then progresses to advanced topics like schema design, performance tuning, and data modeling. A unique and integral component of this curriculum is dedicated instruction on leveraging modern AI tools, specifically Large Language Models (LLMs), as a learning accelerator and pair programmer. This approach recognizes that the landscape of technical learning has evolved, and proficiency with these AI assistants is becoming a critical skill in its own right. By integrating this from the start, the program aims to produce a more efficient, resourceful, and modern data practitioner.1.2 Why SQLite? The Power of a Serverless, File-Based DatabaseThe choice of SQLite as the foundational database engine for this course is a strategic one, designed to maximize learning efficiency by minimizing setup friction. SQLite is a C-language library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine.1 It is, in fact, the most widely deployed database engine in the world, embedded in countless mobile phones, computers, and applications.3The defining characteristic of SQLite is its architecture. Unlike traditional database systems like PostgreSQL or SQL Server, which operate as separate server processes that client applications connect to, SQLite is serverless.1 The entire database—including the table definitions, the data itself, indexes, and triggers—is stored in a single, ordinary disk file.1 This file-based approach offers several profound advantages for learning:Zero Configuration: There is no server to install, configure, or manage. There are no users to create, no permissions to grant, and no network ports to open. The database is ready to use the moment the command-line tool is available.1 This eliminates a significant barrier to entry that often frustrates newcomers, allowing them to focus their cognitive energy on the subject matter: SQL and data principles.Portability: A complete SQLite database is contained within one file. This makes it incredibly easy to copy, share, email, and move between different computers and operating systems, an ideal feature for a hands-on learning environment.Self-Contained: The SQLite library requires no external dependencies, making the installation process simple and robust.By deliberately choosing a serverless engine, this curriculum removes the administrative overhead inherent in traditional database management systems. This allows the learner to get from zero to their first query in minutes, not hours. This rapid start is crucial for building and maintaining learning momentum. SQLite should not be viewed as a "toy" database; it is a professional-grade tool used extensively for local application storage, testing, embedded systems, and as the perfect, isolated sandbox for mastering universal SQL concepts that are directly transferable to more complex, server-based systems.1.3 Why DBeaver? A Universal GUI for All Your Data NeedsThe second component of our modern toolkit is DBeaver, a free, open-source, and cross-platform universal database tool.4 While it is perfectly possible to interact with SQLite exclusively through its command-line interface (CLI), a powerful Graphical User Interface (GUI) like DBeaver provides a more intuitive and productive environment for learning and professional work.The primary strategic advantage of DBeaver is its universality. It is not tied to a single database engine. DBeaver supports a vast array of relational databases (including SQLite, PostgreSQL, MySQL, SQL Server, and Oracle), NoSQL databases (like MongoDB and Cassandra), and cloud data platforms (such as Snowflake and Redshift).4 This aligns with the course's "learn once, apply everywhere" philosophy.While the backend database technology one uses may change multiple times throughout a career, the primary interface for developing queries, exploring schemas, and managing data can remain consistent. Learning to use DBeaver effectively is an investment that pays dividends regardless of future technological shifts. Its key features include:Advanced SQL Editor: Provides syntax highlighting, auto-completion, and query formatting, which helps in writing clean and correct SQL.Database Navigator: Allows for easy visual exploration of database schemas, including tables, columns, indexes, and other objects.Data Import/Export Wizards: Simplifies the process of moving data between the database and external formats like CSV or JSON, a common real-world task.6Cross-Platform Consistency: DBeaver works identically on Windows, macOS, and Linux, ensuring the skills learned are portable across any development environment.9By pairing the simplicity of SQLite with the power and universality of DBeaver, this course provides a toolchain that is both easy to start with and powerful enough for professional use. The skills acquired with this specific combination are durable and broadly applicable across the entire data industry.Chapter 2: Environment Setup: A Step-by-Step GuideThis chapter provides detailed, step-by-step instructions for installing the necessary tools—the SQLite command-line interface (CLI) and the DBeaver graphical user interface (GUI)—on both Windows and Linux operating systems. Following these instructions will result in a complete, functional environment for the rest of the course.2.1 Installing SQLite and DBeaver on Windows 10/11The installation process on Windows involves downloading precompiled binaries for SQLite and using an installer for DBeaver.Installing the SQLite Command-Line ToolCreate a Dedicated Folder: It is best practice to keep the SQLite tools in a single, easily accessible location. Create a new folder on your system, for example, C:\sqlite.1Download SQLite Tools: Navigate to the official SQLite download page at sqlite.org/download.html. In the "Precompiled Binaries for Windows" section, find and download the zip file for the command-line tools. This file is typically named sqlite-tools-win-x64-*.zip for 64-bit systems.4Extract the Files: Open the downloaded zip file and extract its contents directly into the folder you created in step 1 (e.g., C:\sqlite). You should see three executable files: sqlite3.exe, sqldiff.exe, and sqlite3_analyzer.exe.4 The primary tool used in this course is sqlite3.exe.Add SQLite to the System PATH: This is a crucial step that allows you to run the sqlite3 command from any directory in Command Prompt or PowerShell.Press the Windows key and type "environment variables". Select "Edit the system environment variables".In the System Properties window that opens, click the "Environment Variables..." button.10In the "System variables" section at the bottom, find and select the Path variable, then click "Edit...".4Click "New" and enter the full path to your SQLite folder (e.g., C:\sqlite).4Click "OK" on all open windows to save the changes.Verify the Installation: Open a new Command Prompt or PowerShell window (existing windows will not have the updated PATH). Type the following command and press Enter:Bashsqlite3 --version
If the installation was successful, you will see the installed SQLite version number printed to the console.12Installing DBeaver Community EditionDownload the Installer: Go to the official DBeaver download page at dbeaver.io/download/. Click the link for the "Windows (installer)" to download the executable installer.14Run the Installer: Execute the downloaded file. The installation wizard will guide you through the process. The default settings are appropriate for most users.9 DBeaver bundles its own Java runtime environment, so you do not need to install Java separately.9Launch DBeaver: Once the installation is complete, you can launch DBeaver from the Start Menu.For users who prefer package managers, DBeaver can also be installed via Chocolatey with the command choco install dbeaver.142.2 Installing SQLite and DBeaver on Linux (Ubuntu/Debian)On Debian-based Linux distributions like Ubuntu, the installation process is streamlined using the apt package manager.Installing the SQLite Command-Line ToolUpdate Package Lists: Open a terminal and run the following command to ensure your package manager has the latest information:Bashsudo apt update
2Install SQLite: Install the SQLite 3 package with the following command:Bashsudo apt install sqlite3
2Verify the Installation: To confirm that SQLite was installed correctly, run:Bashsqlite3 --version
The terminal should display the installed version number.2Installing DBeaver Community EditionThe recommended method for installing DBeaver on a Debian-based system is to add its official repository. This ensures you can easily receive updates through the standard apt upgrade process.Add the DBeaver GPG Key: This key is used to verify the authenticity of the DBeaver packages.Bashsudo wget -O /usr/share/keyrings/dbeaver.gpg.key https://dbeaver.io/debs/dbeaver.gpg.key
14Add the DBeaver Repository: This command adds the DBeaver repository to your system's list of sources.Bashecho "deb [signed-by=/usr/share/keyrings/dbeaver.gpg.key] https://dbeaver.io/debs/dbeaver-ce /" | sudo tee /etc/apt/sources.list.d/dbeaver.list
14Install DBeaver: Update your package list again to include the new repository, and then install the dbeaver-ce package.Bashsudo apt update && sudo apt install dbeaver-ce
14Launch DBeaver: You can now start DBeaver from your desktop environment's application menu or by running dbeaver-ce & in the terminal.20Alternative installation methods include downloading the .deb package directly from the website or using Snapcraft (sudo snap install dbeaver-ce).92.3 First Connection: Linking DBeaver to an SQLite DatabaseWith both tools installed, the final step is to configure DBeaver to manage an SQLite database.Launch DBeaver: Open the DBeaver application. You may be prompted to create a sample database; you can select "No" for now.Open the New Connection Wizard: In the top menu, navigate to Database > New Database Connection, or click the plug-shaped icon in the toolbar.6Select SQLite: In the wizard, a list of database drivers will appear. Type "SQLite" in the search box and select it from the list. Click "Next".6Download the JDBC Driver: DBeaver communicates with databases using a standard interface called JDBC (Java Database Connectivity). If this is your first time connecting to SQLite, DBeaver will prompt you to download the necessary JDBC driver files. This is a one-time setup. Click "Download" and wait for the process to complete.6Configure the Connection Path: The main connection settings window will appear. The most important field for SQLite is Path. This is where you specify the full path to your database file.For this course, create a dedicated folder for your projects, for example, C:\Users\YourUser\Documents\SQL_Projects on Windows or /home/youruser/sql_projects on Linux.In the Path field, enter the full path to a database file, for example: /home/youruser/sql_projects/bootcamp.db.Crucially, this file does not need to exist yet. If the file is not found, DBeaver and SQLite will automatically create it when the connection is established.6Test and Finish: Click the "Test Connection..." button at the bottom left. If everything is configured correctly, you will see a success message. Click "Finish".Your new database connection will now appear in the "Database Navigator" panel on the left side of the DBeaver window. You can expand it to see its structure.It is important to understand a key distinction between SQLite and server-based databases. While the DBeaver connection wizard has tabs for "SSH" and other network configurations, these are not applicable to a standard local SQLite connection. SQLite is an embedded, file-based database, not a server that listens for network connections.25 Attempting to configure an SSH tunnel to a local file path will not work. For advanced use cases involving the management of an SQLite file on a remote server, one would typically use tools like sshfs to mount the remote directory locally, and then point DBeaver to the local mount point. However, for the entirety of this course, all work will be performed on local database files, simplifying the process and keeping the focus on learning SQL.Part II: The AI-Powered Learning AcceleratorChapter 3: How to Learn with an AI Pair ProgrammerThe advent of powerful Large Language Models (LLMs) like OpenAI's GPT series and Anthropic's Claude has fundamentally changed the landscape of technical learning. When used correctly, these AI models can serve as an incredibly effective learning companion, pair programmer, and Socratic tutor. This chapter provides a framework for interacting with LLMs to accelerate your journey in learning SQL.3.1 The Right Mental Model: Your Fast, Over-Confident AssistantTo use an LLM effectively, it is crucial to adopt the right mental model. These models are not sentient or all-knowing; at their core, they are sophisticated pattern matchers and token predictors.26 They generate text by predicting the most probable next word based on the vast corpus of data they were trained on. This has important implications for how they should be used.The most productive mental model is to think of an LLM as a lightning-fast, over-confident, and sometimes forgetful pair programming assistant.26Lightning-Fast: It can generate boilerplate code, look up syntax, and provide examples in seconds, tasks that would take a human much longer.Over-Confident: It will almost always provide an answer, even if it has to invent one. LLMs can "hallucinate" non-existent functions, incorrect syntax, or subtly flawed logic with the same authoritative tone as a correct answer. You must always verify the AI's output.Forgetful: A standard chat-based LLM has a limited context window. It remembers the current conversation but has no memory of previous chats. You must provide all relevant context within each session.Do not fall into the trap of anthropomorphizing the model. A human who confidently invents a non-existent library would lose all credibility; for an LLM, this is simply a known failure mode of the technology.26 Your role as the learner is not to blindly accept its output, but to guide it, question it, and use its speed to accelerate your own experimentation and iteration cycles. Embrace a process of "vibe-coding"—throwing ideas and prompts at the model to see what works. This rapid feedback loop is a fantastic way to build intuition and discover the capabilities and limitations of both SQL and the AI itself.263.2 Crafting Effective Prompts for Concept ClarificationThe quality of the output you receive from an LLM is directly proportional to the quality of the input you provide. Learning to craft effective prompts is the single most important skill for using AI as a learning tool.Principle 1: Be Specific and ConciseVague questions lead to vague answers. Instead of asking, "How do joins work?", ask a more targeted question. Research suggests that shorter, more concise prompts (under 50-150 words) often lead to higher success rates, as longer prompts can increase the likelihood of the model generating incorrect or irrelevant code.27Principle 2: Provide Rich ContextThis is the most critical element. The LLM does not know what you know or what your database looks like. You must provide it with the necessary context.For Schema Questions: Always provide the CREATE TABLE statements for the relevant tables. This gives the model the exact structure, column names, and data types to work with.28For Query Questions: Provide the full SQL query you are asking about.For Conceptual Questions: Frame the question for your persona. For example, "Explain database normalization to a developer who understands object-oriented programming."Example Prompts:Weak Prompt: "Explain foreign keys."Strong Prompt:"Explain the concept of a foreign key in SQL. Use the following table schemas as an example to show how a foreign key in the orders table would reference the customers table.SQLCREATE TABLE customers (
  customer_id INTEGER PRIMARY KEY,
  email TEXT NOT NULL UNIQUE
);

CREATE TABLE orders (
  order_id INTEGER PRIMARY KEY,
  order_date TEXT NOT NULL,
  amount REAL NOT NULL,
  customer_id INTEGER
);
Also, explain what 'referential integrity' means in this context."3.3 Debugging with AI: How to Ask for Help with Broken CodeWhen you encounter an error, an LLM can be an excellent debugger, but only if you provide it with all the necessary information. Use a structured, three-part prompt for the best results.The Three-Part Debugging Prompt:The Goal: Clearly state what you are trying to achieve.The Code: Paste the exact SQL query that is failing.The Error: Paste the exact error message you are receiving from DBeaver or the sqlite3 CLI.Example Debugging Prompt:Goal: I'm trying to find all employees who are 'Developers' and calculate their average salary.My Code:SQLSELECT department, AVG(salary)
FROM employees
WHERE position = 'Developer'
The Error:Error: near "SELECT": syntax errorCan you identify the error in my SQL query and provide the corrected version?In this case, the AI would quickly identify the missing GROUP BY clause, a common mistake for beginners, and provide the corrected query along with an explanation.3.4 Using AI to Explain and Optimize QueriesLLMs excel at deconstructing complex code and explaining it in simple terms. This is invaluable for learning and for understanding code written by others.Explaining a QueryPaste a query and ask for a breakdown.Prompt:"Please explain this SQL query step-by-step. Describe what each part of the query does, from the JOIN to the GROUP BY and HAVING clauses.SQLSELECT
  c.category_name,
  COUNT(p.product_id) as product_count
FROM Categories c
JOIN Products p ON c.category_id = p.category_id
GROUP BY c.category_name
HAVING COUNT(p.product_id) > 10;
```"
Explaining Query ResultsIf a query runs but the results are not what you expect, provide the query, the schema, and a sample of the output.Prompt:"The following query is supposed to show me customers who have not placed any orders. However, it's returning an empty result set, even though I know such customers exist. Can you explain why my query is failing and suggest a correct version?Schema:CREATE TABLE customers (id INT, name TEXT);CREATE TABLE orders (order_id INT, customer_id INT);Query:SELECT name FROM customers WHERE id NOT IN (SELECT customer_id FROM orders);"Optimizing a Slow QueryLLMs can suggest performance improvements, most commonly by recommending indexes.Prompt:"This query is very slow on my transactions table, which has over 5 million rows. Can you suggest how to optimize it? Specifically, what index or indexes should I create?Schema:CREATE TABLE transactions (id INT, product_id INT, user_id INT, transaction_date TEXT, amount REAL);Slow Query:SELECT user_id, SUM(amount) FROM transactions WHERE transaction_date BETWEEN '2024-01-01' AND '2024-03-31' GROUP BY user_id;"The AI would likely recommend creating an index on the transaction_date column to speed up the WHERE clause filtering.Finally, a powerful technique is to have the LLM help you maintain the context of your session. After you successfully generate a new table or view with the AI's help, you can instruct it: "That's correct. For all future prompts in this conversation, assume the database now includes this new table: ``." This builds a durable and evolving context, making the AI an even more effective learning partner.29Part III: The Core SQL CurriculumChapter 4 (Level 0): What is a Database?Before writing a single line of SQL, it is essential to understand the fundamental concepts of what a database is, what problems it solves, and how it organizes information. This chapter lays the conceptual groundwork for the entire course.4.1 From Spreadsheets to Databases: The "Why"For many, the first exposure to organized data is a spreadsheet application like Microsoft Excel or Google Sheets. A spreadsheet is a powerful tool for simple data tasks, but as data volume, complexity, or the number of users grows, its limitations become apparent.30Consider a small business tracking sales in a spreadsheet. Initially, this works well. But soon, problems emerge:Data Redundancy: If the same customer makes multiple purchases, their name and address are typed in again and again. If they move, their address must be updated in multiple places, creating a high risk of error.Data Integrity Issues: There is no easy way to enforce rules. A typo could enter a product price as "$19.99" in one row and "19.99" in another, making calculations difficult. An order could be entered for a customer_id that doesn't exist.Lack of Concurrency: If two people try to edit the spreadsheet at the same time, they can easily overwrite each other's changes or corrupt the file.Poor Performance: A spreadsheet with hundreds of thousands or millions of rows becomes slow, unwieldy, and prone to crashing.Security Concerns: It is difficult to control who can see or edit specific pieces of information. For example, a sales agent might need to see order details but not employee salaries.A database is a purpose-built system designed to solve these problems. It is a structured collection of data stored electronically, managed by a software system that ensures organization, reliability, security, and performance, even at massive scale.30 A Database Management System (DBMS) is the software that allows users to create, read, update, and manage the data in a database.304.2 Core Concepts: Databases, Tables, Rows, and ColumnsRelational databases, the focus of this course, organize data using a simple yet powerful model based on a few core concepts.Database: A database can be thought of as the main container that holds all the related data and its structural definitions for a particular application or system.34 In the context of SQLite, the entire database is physically stored as a single file on your computer.Table: A table is the primary database object used to store data. It logically organizes information into a two-dimensional format of rows and columns, much like a single sheet in a spreadsheet workbook.35 A database typically contains many tables, each storing a specific type of entity (e.g., a customers table, a products table, and an orders table).34Column (also Field or Attribute): A column represents a single piece of information about an entity and is the vertical component of a table. Each column has a name (e.g., first_name) and a specific data type (e.g., TEXT) that defines what kind of data it can hold.35Row (also Record or Tuple): A row represents a single, complete entry for an entity and is the horizontal component of a table. For example, one row in a customers table would contain all the information for a single customer: their ID, name, email, and so on.35This structure ensures that data is organized logically and consistently, forming the foundation upon which all database operations are built.4.3 Introduction to SQL: The Language of DataSQL (Structured Query Language) is the standard programming language used to communicate with and manage relational databases.32 It is the tool used to ask the database questions, to change the data it holds, and to define the very structure of the tables themselves. While there are different versions or "dialects" of SQL (e.g., T-SQL for Microsoft SQL Server, PL/pgSQL for PostgreSQL), the core commands and syntax are highly standardized, meaning the SQL learned in this course is applicable across nearly all relational database systems.33SQL commands can be conceptually divided into several sub-languages, each with a distinct purpose:Data Query Language (DQL): This is used for retrieving data. The cornerstone of DQL is the SELECT statement, which is the most frequently used command in SQL.Data Manipulation Language (DML): This is used for modifying the data stored in tables. The core DML commands are INSERT (add new rows), UPDATE (change existing rows), and DELETE (remove rows).Data Definition Language (DDL): This is used for defining and managing the structure of the database itself. The primary DDL commands are CREATE (to build new tables, views, etc.), ALTER (to modify existing structures), and DROP (to delete them).Data Control Language (DCL): This is used to manage user access and permissions. Its commands, GRANT and REVOKE, control who can do what within the database.Transaction Control Language (TCL): This is used to manage transactions, which are sequences of operations that must be executed as a single, atomic unit. Its commands, COMMIT and ROLLBACK, ensure data integrity during complex modifications.This course will provide in-depth, practical training in DQL, DML, and DDL, as these form the essential toolkit for any data practitioner.33Chapter 5 (Level 1): Core QueryingThis chapter introduces the fundamental building blocks of SQL: retrieving, filtering, sorting, and limiting data. Mastering these core DQL (Data Query Language) commands is the first and most important step toward SQL proficiency. All operations in this chapter will use the SELECT statement, the workhorse of data retrieval.5.1 The SELECT Statement: Retrieving DataThe SELECT statement is used to query the database and retrieve data that matches criteria that you specify. The most basic form of the statement selects specific columns from a single table.Syntax:SQLSELECT column1, column2,...
FROM table_name;
SELECT: Specifies the columns you want to retrieve.FROM: Specifies the table from which to retrieve the data.For example, to get the names and prices of all items from a products table:SQLSELECT name, price
FROM products;
To retrieve all columns from a table without listing them individually, you can use the asterisk (*) wildcard.Syntax:SQLSELECT *
FROM table_name;
While SELECT * is convenient for initial exploration, it is considered poor practice in production code. It can be inefficient as it may retrieve more data than necessary, and it can cause applications to break if the table structure changes (e.g., a column is added).It is often useful to rename columns in the query output for clarity. This is done using the AS keyword to create an alias.Syntax:SQLSELECT column_name AS alias_name
FROM table_name;
For example:SQLSELECT product_name AS "Product Name", list_price AS "Price"
FROM products;
5.2 Filtering with WHEREThe WHERE clause is used to extract only those records that fulfill a specified condition. It filters rows before any other processing, making queries more efficient and targeted.40Syntax:SQLSELECT column1, column2,...
FROM table_name
WHERE condition;
Comparison Operators:The WHERE clause uses standard comparison operators:=: Equal to!= or <>: Not equal to>: Greater than<: Less than>=: Greater than or equal to<=: Less than or equal toExample: Find all products that cost more than $50.SQLSELECT name, price
FROM products
WHERE price > 50;
Logical Operators:Multiple conditions can be combined using AND and OR.AND: Displays a record if all conditions separated by AND are TRUE.OR: Displays a record if any of the conditions separated by OR are TRUE.Example: Find all products in the 'Electronics' category that cost less than $100.SQLSELECT name, price, category
FROM products
WHERE category = 'Electronics' AND price < 100;
Other Useful WHERE Clause Operators:BETWEEN: Selects values within a given range (inclusive). WHERE price BETWEEN 50 AND 100;IN: Specifies multiple possible values for a column. WHERE category IN ('Electronics', 'Home Goods');LIKE: Searches for a specified pattern in a column. It uses wildcards:%: Represents zero, one, or multiple characters._: Represents a single character.Example: WHERE name LIKE 'Laptop%'; finds all products whose name starts with "Laptop".IS NULL / IS NOT NULL: Checks for empty (NULL) or non-empty values. WHERE description IS NULL;5.3 Sorting with ORDER BY and Limiting with LIMITBy default, a database does not guarantee the order of rows in a query result. The ORDER BY clause is used to sort the result-set in ascending or descending order.42Syntax:SQLSELECT column1, column2,...
FROM table_name
ORDER BY column1 ASC|DESC, column2 ASC|DESC,...;
ASC: Sorts in ascending order (A-Z, 0-9). This is the default.DESC: Sorts in descending order (Z-A, 9-0).Example: List all products from most expensive to least expensive.SQLSELECT name, price
FROM products
ORDER BY price DESC;
The LIMIT clause is used to specify the maximum number of records to return.42 This is essential for performance when dealing with large tables and for tasks like finding the "top N" items.Syntax:SQLSELECT column1, column2,...
FROM table_name
LIMIT number;
Example: Find the top 5 most expensive products.SQLSELECT name, price
FROM products
ORDER BY price DESC
LIMIT 5;
It is important to understand the logical order of execution for these clauses. The database processes a query in a specific sequence: FROM, then WHERE, then SELECT, then ORDER BY, and finally LIMIT.43 This means the data is first filtered by WHERE, then sorted by ORDER BY, and only then is the LIMIT applied to the sorted result.For implementing pagination (e.g., "show results 11-20"), you can use the OFFSET clause with LIMIT. OFFSET skips a specified number of rows before starting to return rows.Example: List products 6 through 10, ordered by name.SQLSELECT name, category
FROM products
ORDER BY name ASC
LIMIT 5 OFFSET 5; -- Skip the first 5, then return the next 5
5.4 Finding Unique Values with DISTINCTThe DISTINCT keyword is used to return only unique (different) values, eliminating duplicate rows from the result set.43Syntax:SQLSELECT DISTINCT column1, column2,...
FROM table_name;
When used on a single column, DISTINCT returns a list of all unique values in that column.Example: Get a list of all unique product categories.SQLSELECT DISTINCT category
FROM products;
When used on multiple columns, DISTINCT returns the unique combinations of values across those columns.46Example: Find all unique combinations of category and brand.SQLSELECT DISTINCT category, brand
FROM products;
This query would return ('Electronics', 'Sony') as one row and ('Electronics', 'Samsung') as another, but it would not return ('Electronics', 'Sony') twice, even if there are many Sony products in the Electronics category.5.5 Mini-Project: Basic Data ExplorationThis project uses the e-commerce products sample dataset.Challenge:Find the names and prices of all products in the 'Clothing' category that cost more than $50.Find the unique brand names of products available in the store.List the top 5 cheapest products (lowest price), ordered alphabetically by name for any products that have the same price.Answer Key:Query:SQLSELECT name, price
FROM products
WHERE category = 'Clothing' AND price > 50;
Explanation: This query uses a WHERE clause with an AND operator to filter for rows that meet both conditions: the category must be 'Clothing' and the price must be greater than 50.Query:SQLSELECT DISTINCT brand
FROM products
ORDER BY brand;
Explanation: SELECT DISTINCT on the brand column retrieves each brand name only once, eliminating duplicates. ORDER BY is added to present the list in a clean, alphabetical format.Query:SQLSELECT name, price
FROM products
ORDER BY price ASC, name ASC
LIMIT 5;
Explanation: This query first sorts all products by price in ascending order (ASC). For any products with the same price, it then applies a secondary sort by name alphabetically. Finally, LIMIT 5 selects only the first five rows from this sorted result.Chapter 6 (Level 2): Connecting Data with JOINsIn a well-designed relational database, information is split across multiple tables to avoid redundancy and improve data integrity. For example, instead of storing customer details with every single order, we have a customers table and an orders table. The JOIN clause is the powerful SQL mechanism that lets us temporarily combine these tables back together to answer meaningful questions.6.1 The Relational Model: How Tables ConnectThe "relation" in a relational database is the link between tables. This link is established using keys:Primary Key (PK): A column (or set of columns) in a table that contains a unique value for each row. This is the row's unique identifier. For example, customer_id in the customers table.Foreign Key (FK): A column in one table that refers to the Primary Key of another table. This creates the link. For example, the orders table would have a customer_id column that holds a value corresponding to a customer_id in the customers table.JOIN operations use these key relationships to match rows from different tables.6.2 INNER JOIN: Finding Matching RecordsThe INNER JOIN is the most common type of join. It returns only the rows that have a matching key in both tables being joined. If a row in one table does not have a corresponding match in the other, it is excluded from the result set.47Visually, an INNER JOIN can be represented by the intersection of two sets in a Venn diagram.50Syntax:SQLSELECT table1.column1, table2.column2
FROM tableA AS t1
INNER JOIN tableB AS t2
  ON t1.primary_key = t2.foreign_key;
INNER JOIN: Specifies the table to join with. JOIN can be used as a shorthand for INNER JOIN.ON: Specifies the join condition—how the tables are related. This is almost always a comparison between the PK of one table and the FK of the other.Table aliases (t1, t2) are used to shorten table names and are essential for clarity when column names are the same across tables (e.g., t1.id, t2.id).Example: Get a list of all customer names and the IDs of the orders they have placed.SQLSELECT
  c.name,
  o.order_id
FROM customers AS c
INNER JOIN orders AS o
  ON c.customer_id = o.customer_id;
This query will only return customers who have placed at least one order. Customers without orders will not appear in the result.6.3 LEFT and RIGHT JOIN: Keeping All Data from One TableSometimes you need to see all the data from one table, regardless of whether it has a match in the other. This is where outer joins come in.LEFT JOIN (or LEFT OUTER JOIN)A LEFT JOIN returns all rows from the left table (the one mentioned after FROM) and the matched rows from the right table (the one mentioned after JOIN). If there is no match for a row from the left table, the columns from the right table will contain NULL values.47This is extremely useful for finding records that do not have a match. For example, "Show me all customers, and if they have placed an order, show the order ID."Syntax:SQLSELECT t1.column1, t2.column2
FROM tableA AS t1
LEFT JOIN tableB AS t2
  ON t1.primary_key = t2.foreign_key;
Example: Find all customers and the IDs of any orders they may have placed.SQLSELECT
  c.name,
  o.order_id
FROM customers AS c
LEFT JOIN orders AS o
  ON c.customer_id = o.customer_id;
In this result, customers who have never ordered will still appear, but their order_id will be NULL. This allows us to answer the question, "Which customers have never placed an order?"SQLSELECT c.name
FROM customers AS c
LEFT JOIN orders AS o
  ON c.customer_id = o.customer_id
WHERE o.order_id IS NULL;
RIGHT JOIN (or RIGHT OUTER JOIN)A RIGHT JOIN is the inverse of a LEFT JOIN. It returns all rows from the right table and the matched rows from the left table. If there is no match, the left-side columns are NULL.47 In practice, RIGHT JOIN is used infrequently because any RIGHT JOIN can be rewritten as a LEFT JOIN simply by swapping the order of the tables, which is often more intuitive to read.496.4 FULL OUTER JOIN: Combining EverythingA FULL OUTER JOIN combines the results of both LEFT and RIGHT joins. It returns all rows from both tables. It will place NULL values on the side where a match is not found.47 This is useful when you want to see all records from two tables and see where the overlaps and non-overlaps occur.Important SQLite Limitation:Standard SQLite does not support RIGHT JOIN or FULL OUTER JOIN. This is a critical, practical detail to remember when working with this engine. However, these joins can be emulated using LEFT JOIN and UNION ALL.Emulating a FULL OUTER JOIN in SQLite:SQL-- Select all from the left table, with matches from the right
SELECT * FROM tableA
LEFT JOIN tableB ON tableA.id = tableB.id
UNION ALL
-- Select all from the right table that were NOT matched in the first query
SELECT * FROM tableB
LEFT JOIN tableA ON tableA.id = tableB.id
WHERE tableA.id IS NULL;
This advanced technique demonstrates how to achieve the same result with the available tools, reinforcing a deeper understanding of SQL logic.6.5 CROSS JOIN: The Cartesian ProductA CROSS JOIN returns the Cartesian product of the two tables involved. This means that every row from the first table is paired with every row from the second table. No ON clause is needed.47Syntax:SQLSELECT *
FROM tableA
CROSS JOIN tableB;
If tableA has 10 rows and tableB has 20 rows, the CROSS JOIN will produce 10×20=200 rows. This is generally not used for transactional data but can be useful for generating all possible combinations of two sets, such as creating a list of all possible t-shirt variations from a sizes table and a colors table.6.6 Mini-Project: Combining Customer and Order DataThis project uses the e-commerce customers and orders sample datasets.Challenge:Generate a report showing the email address of each customer and the date of every order they placed. The report should be ordered by order date, from most recent to oldest.The marketing team wants to send a special offer to new customers who haven't made a purchase yet. Generate a list of email addresses for all customers who have never placed an order.Answer Key:Query:SQLSELECT
  c.email,
  o.order_date
FROM customers AS c
INNER JOIN orders AS o
  ON c.customer_id = o.customer_id
ORDER BY o.order_date DESC;
Explanation: An INNER JOIN is appropriate here because we only want to see customers who have actually placed orders. We join the tables on the customer_id and order the results by order_date descending to show the newest orders first.Query:SQLSELECT c.email
FROM customers AS c
LEFT JOIN orders AS o
  ON c.customer_id = o.customer_id
WHERE o.order_id IS NULL;
Explanation: A LEFT JOIN is essential for this task. It starts with all customers and then tries to match them with orders. For customers who have no orders, the columns from the orders table (like o.order_id) will be NULL. The WHERE o.order_id IS NULL clause then filters the result set to include only those customers, effectively identifying those who have never made a purchase.Table: SQL JOIN Types at a GlanceJOIN TypeVenn DiagramDescriptionCommon Use CaseSupported in SQLite?INNER JOINReturns only the rows where the join condition is met in both tables.Find all customers who have placed orders.YesLEFT JOINReturns all rows from the left table, and matched rows from the right table. Unmatched rows on the right are NULL.Find all customers, including those who have not placed orders.YesRIGHT JOIN!(https://i.imgur.com/Tj2dE5E.png)Returns all rows from the right table, and matched rows from the left table. Unmatched rows on the left are NULL.Find all products that have been ordered, including products with no matching category info.No (Emulate with LEFT JOIN)FULL OUTER JOINReturns all rows from both tables, placing NULLs where there is no match on either side.Show a complete list of all employees and all departments, matching them where possible.No (Emulate with LEFT JOIN and UNION ALL)Chapter 7 (Level 3): Aggregating and Summarizing DataRaw data is useful, but true insights come from summarizing and aggregating it. This chapter focuses on SQL's aggregate functions and the GROUP BY clause, which together form the foundation of data analysis. These tools allow you to move from viewing individual rows to calculating summary statistics like totals, averages, and counts across groups of data.7.1 Introduction to Aggregate FunctionsAggregate functions perform a calculation on a set of rows and return a single, summary value.51 They are typically used in the SELECT list or in a HAVING clause.The most common aggregate functions are:COUNT(): Counts the number of rows.COUNT(*): Counts the total number of rows in a group or table.COUNT(column_name): Counts the number of rows where column_name is not NULL.COUNT(DISTINCT column_name): Counts the number of unique, non-NULL values in a column.SUM(): Calculates the sum of all numeric values in a column. NULL values are ignored.AVG(): Calculates the average of all numeric values in a column. NULL values are ignored.MIN(): Returns the minimum value in a column.MAX(): Returns the maximum value in a column.When used without a GROUP BY clause, these functions operate on the entire table and return a single row.Example: Get overall statistics for a products table.SQLSELECT
  COUNT(*) AS total_products,
  AVG(price) AS average_price,
  MAX(price) AS most_expensive
FROM products;
7.2 Grouping with GROUP BYThe real power of aggregate functions is unlocked when they are combined with the GROUP BY clause. GROUP BY arranges identical data into groups, allowing you to apply aggregate functions to each group independently.51This process follows a split-apply-combine strategy 54:Split: The dataset is divided into groups based on the unique values in the column(s) specified in the GROUP BY clause.Apply: The aggregate function (e.g., COUNT(), SUM()) is applied to each group.Combine: The results for each group are combined into the final result set, with one row per group.Syntax:SQLSELECT column_to_group_by, aggregate_function(column_to_aggregate)
FROM table_name
GROUP BY column_to_group_by;
The Fundamental Rule of GROUP BY:When using a GROUP BY clause, any column in the SELECT list that is not an aggregate function must be included in the GROUP BY clause. This is a logical requirement. If you were to group by category and ask for the name column, the database would not know which specific product name to show for the entire 'Electronics' category.Example 1: Grouping by a Single ColumnCount the number of products in each category.SQLSELECT
  category,
  COUNT(*) AS number_of_products
FROM products
GROUP BY category;
Example 2: Grouping by Multiple ColumnsCalculate the average price for each brand within each category.SQLSELECT
  category,
  brand,
  AVG(price) AS average_price
FROM products
GROUP BY category, brand;
7.3 Filtering Groups with HAVINGWhile the WHERE clause filters individual rows, the HAVING clause is used to filter entire groups after the GROUP BY clause and aggregate functions have been applied.53The Critical Difference: WHERE vs. HAVINGThis is a common point of confusion, and understanding the logical order of operations is key:FROM: The table is specified.WHERE: Individual rows are filtered based on a condition.GROUP BY: The remaining rows are grouped.HAVING: The newly formed groups are filtered based on an aggregate condition.SELECT: The final columns are selected.ORDER BY: The final result set is sorted.In short: WHERE filters rows, HAVING filters groups. You cannot use an aggregate function in a WHERE clause, but you can in a HAVING clause.Syntax:SQLSELECT column_to_group_by, aggregate_function(column_to_aggregate)
FROM table_name
WHERE row_condition
GROUP BY column_to_group_by
HAVING group_condition;
Example: Find product categories that have more than 10 products and an average price over $50.SQLSELECT
  category,
  COUNT(*) AS product_count,
  AVG(price) AS average_price
FROM products
GROUP BY category
HAVING COUNT(*) > 10 AND AVG(price) > 50;
Example with both WHERE and HAVING:List the total sales revenue for each customer from the 'USA', but only for customers who have spent more than $1,000 in total.SQLSELECT
  c.customer_name,
  SUM(o.total_amount) AS total_spent
FROM customers AS c
JOIN orders AS o ON c.customer_id = o.customer_id
WHERE c.country = 'USA' -- Filter rows before grouping
GROUP BY c.customer_name
HAVING SUM(o.total_amount) > 1000; -- Filter groups after aggregation
7.4 Mini-Project: Sales and University Enrollment ReportsDatasets:E-commerce: products, order_itemsUniversity: students, courses, enrollmentsChallenges:(E-commerce) Calculate the total number of units sold for each product. The report should show the product name and the total quantity sold, ordered by the quantity sold in descending order.(E-commerce) Identify the "big ticket" products. Find the products where the average price of a single item in an order (price from the order_items table) is greater than $200.(University) Generate a report showing the number of students enrolled in each course. The report should display the course name and the student count.(University) The university wants to identify under-enrolled courses that may be cancelled. List the names of all courses that have fewer than 5 students enrolled.Answer Key:Query:SQLSELECT
  p.name,
  SUM(oi.quantity) AS total_units_sold
FROM products AS p
JOIN order_items AS oi ON p.product_id = oi.product_id
GROUP BY p.name
ORDER BY total_units_sold DESC;
Explanation: This query joins products and order_items, groups by the product name, and uses SUM(oi.quantity) to calculate the total sold for each product. ORDER BY... DESC lists the best-selling products first.Query:SQLSELECT
  p.name
FROM products AS p
JOIN order_items AS oi ON p.product_id = oi.product_id
GROUP BY p.name
HAVING AVG(oi.price) > 200;
Explanation: This query groups the items by product name and then uses a HAVING clause with AVG() to filter for only those groups (products) where the average price per line item exceeds $200.Query:SQLSELECT
  c.course_name,
  COUNT(e.student_id) AS student_count
FROM courses AS c
JOIN enrollments AS e ON c.course_id = e.course_id
GROUP BY c.course_name;
Explanation: This query joins courses and enrollments, groups by the course name, and uses COUNT(e.student_id) to count the number of students in each course group.Query:SQLSELECT
  c.course_name
FROM courses AS c
JOIN enrollments AS e ON c.course_id = e.course_id
GROUP BY c.course_name
HAVING COUNT(e.student_id) < 5;
Explanation: Building on the previous query, this adds a HAVING clause to filter the grouped results, showing only those courses where the student count is less than 5.Chapter 8 (Level 4): Data Modification Language (DML)While querying data with SELECT is a critical skill, a complete data practitioner must also know how to modify data. SQL's Data Manipulation Language (DML) provides three core commands for this purpose: INSERT to add new data, UPDATE to change existing data, and DELETE to remove it. These commands are powerful and must be used with caution.8.1 Adding Data with INSERTThe INSERT INTO statement is used to add new rows of data to a table.57Syntax 1: Specifying Columns (Recommended)This is the safest and most explicit method. You specify the columns you are providing data for, and the values in the corresponding order.SQLINSERT INTO table_name (column1, column2, column3)
VALUES (value1, value2, value3);
58Example: Add a new record to the employees table.SQLINSERT INTO employees (employee_id, first_name, last_name, department)
VALUES (101, 'Alice', 'Smith', 'Engineering');
Syntax 2: Inserting Multiple RowsTo insert multiple rows at once, which is far more efficient than running separate INSERT statements, you can provide multiple sets of values.SQLINSERT INTO table_name (column1, column2)
VALUES
  (value1a, value2a),
  (value1b, value2b),
  (value1c, value2c);
Syntax 3: Inserting Data from Another TableThe INSERT INTO... SELECT statement allows you to copy data from one table and insert it into another. The data types of the source and target columns must be compatible.SQLINSERT INTO new_employees (first_name, last_name)
SELECT first_name, last_name
FROM candidate_pool
WHERE status = 'Hired';
588.2 Changing Data with UPDATEThe UPDATE statement is used to modify existing records in a table.57Syntax:SQLUPDATE table_name
SET column1 = value1, column2 = value2,...
WHERE condition;
SET: Specifies the column(s) to modify and their new value(s). You can use calculations, such as SET salary = salary * 1.05.WHERE: Specifies which row(s) to update.Critical Safety Warning: The WHERE clause is not technically required, but its omission is extremely dangerous. If you execute an UPDATE statement without a WHERE clause, it will modify every single row in the table..58Best Practice: Before running an UPDATE, write a SELECT statement with the exact same WHERE clause to preview which rows will be affected.SQL-- First, preview the rows to be updated
SELECT * FROM employees WHERE employee_id = 101;

-- If the preview is correct, run the update
UPDATE employees
SET department = 'Data Science'
WHERE employee_id = 101;
8.3 Removing Data with DELETEThe DELETE statement is used to remove existing rows from a table.57Syntax:SQLDELETE FROM table_name
WHERE condition;
Critical Safety Warning: Just like UPDATE, the WHERE clause is optional but vital. Executing DELETE FROM table_name without a WHERE clause will permanently remove all data from the table..59 Always use the SELECT preview technique to verify the rows you intend to delete before executing the command.Example: Remove an employee who has left the company.SQL-- First, preview the row to be deleted
SELECT * FROM employees WHERE employee_id = 101;

-- If correct, run the delete
DELETE FROM employees
WHERE employee_id = 101;
In larger database systems like SQL Server or PostgreSQL, the TRUNCATE TABLE command exists as a faster way to delete all rows from a table. It doesn't log individual row deletions, making it more efficient but also less safe as it typically cannot be rolled back.57 SQLite does not have a separate TRUNCATE command; DELETE without a WHERE clause performs this function.8.4 Mini-Project: Managing Patient RecordsThis project uses the healthcare patients and visits tables.Challenge:A new patient, 'Maria Garcia', born on '1985-05-22', has checked in. Her patient ID is 5001. INSERT her record into the patients table.Patient 'John Smith' (patient_id 1010) has informed the clinic that his date of birth was entered incorrectly. It should be '1970-02-15'. UPDATE his record.A duplicate patient record for patient ID 9999 was created in error. DELETE this record from the patients table.Answer Key:INSERT Query:SQLINSERT INTO patients (patient_id, first_name, last_name, date_of_birth)
VALUES (5001, 'Maria', 'Garcia', '1985-05-22');
Explanation: This statement adds a new row to the patients table, specifying the values for four columns.UPDATE Query:SQLUPDATE patients
SET date_of_birth = '1970-02-15'
WHERE patient_id = 1010;
Explanation: This statement finds the row where patient_id is 1010 and changes the value in the date_of_birth column to the new, correct date.DELETE Query:SQLDELETE FROM patients
WHERE patient_id = 9999;
Explanation: This statement finds and permanently removes the row where the patient_id is 9999.Chapter 9 (Level 5): Database Schema DesignSchema design is the blueprint for a database. It is the process of planning and defining the structure of the tables, the columns within them, the relationships between tables, and the rules that govern the data. A well-designed schema is the foundation of a robust, efficient, and reliable database. This chapter covers the DDL (Data Definition Language) commands and design principles needed to build that foundation.9.1 Creating Tables with CREATE TABLEThe CREATE TABLE statement is the fundamental DDL command used to define a new table in the database.60Syntax:SQLCREATE TABLE table_name (
  column1_name data_type [constraints],
  column2_name data_type [constraints],
 ...
  table_constraints
);
SQLite Data Types:SQLite uses a more flexible typing system called "type affinity" than many other databases. While you can declare various standard SQL types, SQLite will internally store them as one of the following core types 38:INTEGER: A whole number. Can be used for auto-incrementing primary keys.REAL: A floating-point (decimal) number.TEXT: A text string.BLOB: Binary Large Object, used for storing data exactly as it was input (e.g., images, files).NUMERIC: Can hold values of any of the above types.For clarity and compatibility, it is best practice to use the standard types (INTEGER, TEXT, REAL) in your CREATE TABLE statements.9.2 Constraints: Enforcing Data RulesConstraints are rules applied to columns to enforce data integrity and ensure the data is valid and consistent.33NOT NULL: Ensures that a column cannot have a NULL (empty) value. A value must be provided for this column in every row.UNIQUE: Ensures that every value in a column (or combination of columns) is unique across all rows in the table.DEFAULT: Provides a default value for a column if no value is specified during an INSERT. For example, status TEXT DEFAULT 'active'.CHECK: Enforces a custom rule on the values in a column. The rule must evaluate to a boolean TRUE or FALSE. For example, price REAL CHECK (price > 0).PRIMARY KEY: A combination of NOT NULL and UNIQUE. This constraint identifies the column(s) that will uniquely identify each row in the table.FOREIGN KEY: Creates a link to a primary key in another table, enforcing referential integrity.9.3 Primary Keys: The Unique IdentifierAs previously discussed, a Primary Key (PK) is a constraint that uniquely identifies each record in a table.30 A good PK should be unique, unchanging, and never NULL.60In most cases, the best practice is to use a surrogate key: an artificial key that has no business meaning but serves only to uniquely identify a row. An auto-incrementing integer is the most common type of surrogate key.In SQLite, this is easily achieved with the following syntax:SQLCREATE TABLE products (
  product_id INTEGER PRIMARY KEY, -- This creates an auto-incrementing integer PK
  name TEXT NOT NULL
);
When you INSERT a new product without specifying a product_id, SQLite will automatically assign the next available integer.A composite primary key is a primary key made up of two or more columns. This is often used in linking tables.SQLCREATE TABLE order_items (
  order_id INTEGER,
  product_id INTEGER,
  quantity INTEGER,
  PRIMARY KEY (order_id, product_id)
);
9.4 Foreign Keys: Enforcing Referential IntegrityA Foreign Key (FK) is a column or a set of columns in one table that refers to the Primary Key in another table. This is the mechanism that creates and enforces relationships between tables.30Referential integrity means that if a value exists in a foreign key column, the corresponding value must also exist in the referenced primary key column. This prevents "orphaned" records, such as an order that belongs to a non-existent customer.Syntax:SQLCREATE TABLE orders (
  order_id INTEGER PRIMARY KEY,
  customer_id INTEGER NOT NULL,
  order_date TEXT NOT NULL,
  FOREIGN KEY (customer_id) REFERENCES customers (customer_id)
    ON DELETE RESTRICT
    ON UPDATE CASCADE
);
FOREIGN KEY (customer_id): Specifies the column in the orders table that is the FK.REFERENCES customers (customer_id): Specifies the parent table and its PK that the FK points to.ON DELETE and ON UPDATE Actions:These clauses define what should happen to the child rows (in orders) if the parent row (in customers) is deleted or its primary key is updated.RESTRICT: Prevents the deletion/update of the parent row if any child rows exist. (Default behavior).CASCADE: Deletes/updates the child rows automatically when the parent row is deleted/updated.SET NULL: Sets the foreign key column(s) in the child rows to NULL.SET DEFAULT: Sets the foreign key column(s) to their default value.9.5 Normalization: Designing for Data IntegrityNormalization is a systematic process of organizing the columns and tables in a relational database to minimize data redundancy and improve data integrity.60 It is best understood as a practical technique for preventing common data problems known as anomalies.32Imagine a single, flat table for orders that looks like this:OrderIDOrderDateCustIDCustEmailItemIDItemNameItemPriceQty1012024-01-051alice@email.comP1Laptop120011022024-01-062bob@email.comP2Mouse2521032024-01-071alice@email.comP2Mouse251This design suffers from several anomalies:Update Anomaly: If Alice changes her email, we must find and update it in every single order she has ever placed. Missing even one leads to inconsistent data.Insertion Anomaly: We cannot add a new customer to our system until they place their first order. There is no place to store customer information without an associated order.Deletion Anomaly: If Bob returns his only order and we delete row 102, we lose all information that Bob ever existed as a customer.Normalization solves these problems by breaking the data into smaller, well-structured tables. The process is defined by a series of "Normal Forms."First Normal Form (1NF):Each cell must hold a single, atomic value. No lists or repeating groups in a single cell.Each row must be unique, which is ensured by a primary key.Our example table already satisfies 1NF.Second Normal Form (2NF):The table must be in 1NF.All non-key attributes must be fully functionally dependent on the entire primary key. This rule primarily addresses issues in tables with composite primary keys. If an attribute only depends on part of the composite key, it should be moved to a separate table.Third Normal Form (3NF):The table must be in 2NF.There should be no transitive dependencies. This means that every non-key attribute must depend only on the primary key, and not on any other non-key attribute.In our example, CustEmail depends on CustID, and ItemName and ItemPrice depend on ItemID. These are transitive dependencies. To achieve 3NF, we break the table apart:Customers Table:| CustID (PK) | CustEmail ||---|---|Products Table:| ItemID (PK) | ItemName | ItemPrice ||---|---|---|Orders Table:| OrderID (PK) | OrderDate | CustID (FK) ||---|---|---|Order_Items (Linking Table):| OrderID (FK) | ItemID (FK) | Qty ||---|---|---|(Here, (OrderID, ItemID) forms a composite primary key)This 3NF design eliminates all the anomalies. For most transactional databases (OLTP), achieving 3NF is the standard goal.9.6 Mini-Project: Designing a Simple SchemaScenario:You are tasked with designing a database for a simple online blog. The blog has Authors and Posts. An author can write many posts, but each post is written by only one author.Challenge:Identify the entities, their attributes, and the relationship between them.Determine the primary keys and foreign keys needed to link the tables.Write the CREATE TABLE statements for the necessary tables, including appropriate constraints.Answer Key:Entities and Attributes:Authors: author_id, first_name, last_name, emailPosts: post_id, title, content, publish_date, author_idRelationship: One-to-many. One Author can have many Posts.Keys:authors table: author_id is the Primary Key.posts table: post_id is the Primary Key. author_id is the Foreign Key that references the authors table.CREATE TABLE Statements:SQLCREATE TABLE authors (
  author_id INTEGER PRIMARY KEY,
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  email TEXT NOT NULL UNIQUE
);

CREATE TABLE posts (
  post_id INTEGER PRIMARY KEY,
  title TEXT NOT NULL,
  content TEXT,
  publish_date TEXT NOT NULL,
  author_id INTEGER NOT NULL,
  FOREIGN KEY (author_id) REFERENCES authors (author_id)
);
Chapter 10 (Level 6): Data InterchangeA core task for any data practitioner is moving data into and out of a database. Whether it's loading initial data, exporting results for a report, or integrating with other systems, being proficient with data interchange formats like CSV is essential.10.1 Working with CSV FilesCSV (Comma-Separated Values) is a ubiquitous plain-text format for storing tabular data. Each line in the file represents a row, and commas separate the values in each row.62 SQLite and DBeaver both provide excellent support for working with CSV files.Using the sqlite3 Command-Line ToolThe sqlite3 CLI has special "dot-commands" (commands that start with a period) to control its behavior for data interchange.Importing a CSV File:This process loads data from a CSV file into a new or existing SQLite table.Start the CLI and open your database: sqlite3 my_database.dbSet the mode to CSV: .mode csv 62(Optional) If your CSV uses a different separator, like a tab or semicolon, specify it: .separator "\t"Import the data: .import /path/to/your/file.csv table_name 62If table_name does not exist, SQLite will create it and use the first row of the CSV as column headers. All columns will be created with the TEXT data type.If table_name already exists, the CSV data will be appended to it. Important: If the CSV has a header row, this will be imported as a data row, which is usually not desired. To prevent this, use the --skip 1 flag (in newer versions of SQLite) or manually remove the header row from the CSV file first.63Exporting a Query Result to a CSV File:This process runs a SELECT query and writes the output directly to a CSV file.Start the CLI and open your database: sqlite3 my_database.dbTurn on headers to include column names in the output: .headers on 64Set the output mode to CSV: .mode csv 65Redirect output to a file: .output /path/to/your/output.csv 64Execute your query: SELECT * FROM products WHERE category = 'Electronics';The results will be written to output.csv instead of the screen.To stop redirecting output, you can use .output stdout or simply quit the CLI with .quit.66Using DBeaver's GUI WizardsDBeaver provides user-friendly wizards that make these tasks even simpler.To Import Data: In the Database Navigator, right-click on the target table (or the "Tables" folder if you want DBeaver to create the table) and select "Import Data".6 A wizard will open, allowing you to select your CSV file, configure mappings between CSV columns and table columns, and manage data type conversions.To Export Data: Run any SELECT query in the SQL Editor. In the results panel at the bottom, right-click anywhere on the result grid and select "Export Data".6 A wizard will guide you through choosing an output format (CSV, JSON, SQL, etc.), configuring options, and saving the file.10.2 Working with JSON Data in SQLiteModern applications often deal with semi-structured data, and JSON (JavaScript Object Notation) is the de facto standard. SQLite includes a powerful set of built-in functions for storing, parsing, and querying JSON data stored within TEXT columns. While a deep dive is beyond the scope of this core curriculum, it is important to be aware of this capability.The most common function is json_extract(), which allows you to pull a specific value from a JSON string.Example:Imagine a users table with a profile column containing JSON data.SQLCREATE TABLE users (
  user_id INTEGER PRIMARY KEY,
  profile TEXT
);

INSERT INTO users (user_id, profile) VALUES
(1, '{"name": "Alice", "contact": {"email": "alice@example.com", "phone": "555-1234"}}');
To extract the user's email, you can use json_extract():SQLSELECT
  json_extract(profile, '$.contact.email') AS email
FROM users
WHERE user_id = 1;
This demonstrates how SQLite can handle more than just traditional relational data, making it a versatile tool for a wide range of applications.10.3 Mini-Project: Populating a DatabaseThis project synthesizes schema creation and data importation.Dataset:The raw CSV files for the University domain are provided: students.csv, courses.csv, and enrollments.csv.Challenge:Write and execute the CREATE TABLE statements to build the schema for the students, courses, and enrollments tables. Ensure primary and foreign keys are correctly defined to link them.Using either the sqlite3 CLI or the DBeaver GUI wizard, import the data from the three provided CSV files into their corresponding tables.Write and execute a SELECT query that joins the three tables to verify that the data was imported and linked correctly. For example, list the first 10 students' names, the names of the courses they are enrolled in, and their grades.Answer Key:Schema (CREATE TABLE statements):SQLCREATE TABLE students (
  student_id INTEGER PRIMARY KEY,
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  major TEXT
);

CREATE TABLE courses (
  course_id INTEGER PRIMARY KEY,
  course_name TEXT NOT NULL,
  credits INTEGER
);

CREATE TABLE enrollments (
  enrollment_id INTEGER PRIMARY KEY,
  student_id INTEGER NOT NULL,
  course_id INTEGER NOT NULL,
  grade TEXT,
  FOREIGN KEY (student_id) REFERENCES students (student_id),
  FOREIGN KEY (course_id) REFERENCES courses (course_id)
);
Import (CLI Example for students.csv):.mode csv.import --skip 1 students.csv students```(Repeat for courses.csv and enrollments.csv)Verification Query:SQLSELECT
  s.first_name,
  s.last_name,
  c.course_name,
  e.grade
FROM students AS s
JOIN enrollments AS e ON s.student_id = e.student_id
JOIN courses AS c ON e.course_id = c.course_id
LIMIT 10;
Executing this query should produce a result set with 10 rows, confirming that the tables were created, populated, and correctly related.Chapter 11 (Level 7): Performance and OptimizationWriting a correct SQL query is only half the battle. In the real world, with tables containing millions or even billions of rows, writing a performant query is just as important. A poorly optimized query can take hours to run, while a well-optimized one can return the same result in seconds. This chapter introduces the fundamentals of SQL performance tuning, focusing on indexes and query plan analysis.11.1 The Role of Indexes: Speeding Up QueriesAn index is a separate data structure on disk that provides a fast lookup mechanism for the data in a table. It works much like the index in the back of a book: instead of scanning every page (every row) to find a topic, you can go to the index, find the topic, and be pointed directly to the correct page numbers (row locations).39Indexes are most effective when created on columns that are frequently used in WHERE clauses or in JOIN conditions (ON clauses), as these are the operations that involve searching for specific rows.67The Performance Trade-off:Reads (SELECT): Indexes dramatically speed up data retrieval queries.Writes (INSERT, UPDATE, DELETE): Indexes slow down data modification operations. When you insert, update, or delete a row, the database must not only modify the table data but also update every index that contains that row.Because of this trade-off, you should not index every column. You should create indexes strategically on columns used for filtering and joining to optimize your most common and performance-critical queries.Syntax:SQL-- Create an index on a single column
CREATE INDEX index_name ON table_name (column_name);

-- Create a composite index on multiple columns
CREATE INDEX idx_orders_customer_date ON orders (customer_id, order_date);
11.2 Analyzing Query Performance with EXPLAIN QUERY PLANTo optimize a query, you first need to understand how the database is executing it. The EXPLAIN QUERY PLAN command is a diagnostic tool that shows the high-level strategy, or "query plan," that SQLite will use to run your query.68 It makes the invisible work of the database visible.Syntax:SQLEXPLAIN QUERY PLAN
SELECT... FROM... WHERE...;
The key to understanding this tool is to see a "before and after" comparison.Scenario: Querying a Large Table without an IndexImagine a table orders with 1 million rows and you run the following query:SQLEXPLAIN QUERY PLAN
SELECT * FROM orders WHERE customer_id = 12345;
The output would look something like this:QUERY PLAN
`--SCAN TABLE orders
The keyword SCAN TABLE is critical. It means that SQLite is performing a full table scan—it is reading every single one of the 1 million rows from top to bottom to find the ones where customer_id is 12345. This is highly inefficient.Scenario: Querying with an IndexNow, let's create an index on the customer_id column and run the analysis again.SQLCREATE INDEX idx_orders_customer_id ON orders (customer_id);

EXPLAIN QUERY PLAN
SELECT * FROM orders WHERE customer_id = 12345;
The new output will be different:QUERY PLAN
`--SEARCH TABLE orders USING INDEX idx_orders_customer_id (customer_id=?)
The keyword SEARCH TABLE... USING INDEX means that SQLite did not scan the whole table. Instead, it used the idx_orders_customer_id index to jump directly to the locations of the rows for customer 12345, making the query thousands of times faster. This tangible difference in the query plan is the key to understanding and verifying the power of indexes.11.3 SQLite-Specific TuningBeyond standard indexes, SQLite offers a few specific commands for performance tuning.ANALYZE: This command gathers statistics about the data distribution within tables and indexes (e.g., how many unique values a column has). The query planner then uses these statistics to make more intelligent choices about which index to use or how to order joins. It is good practice to run ANALYZE; after creating new indexes or making significant changes to the data in a table.71PRAGMA: PRAGMA commands are used to modify the operation of the SQLite library or to query its internal state. One of the most common performance-related pragmas is PRAGMA journal_mode = WAL;. Enabling Write-Ahead Logging (WAL) can significantly improve performance in applications with concurrent readers and writers, as it allows reads to proceed while writes are happening.7111.4 Mini-Project: Optimizing a Slow QueryThis project uses a pre-generated large e-commerce orders database file (e.g., 1 million rows).Challenge:You are given a query to find all orders placed on a specific date: SELECT * FROM orders WHERE order_date = '2024-07-10';. Execute this query and note how long it takes to run. (DBeaver shows execution time in the status bar).Use EXPLAIN QUERY PLAN to analyze the query. Observe the SCAN TABLE operation.Based on the query and the plan, create an index that you believe will improve performance.Run the SELECT query again and measure the new, improved execution time.Run EXPLAIN QUERY PLAN again to confirm that your index is being used (SEARCH TABLE... USING INDEX).Answer Key:Execution will be slow (several seconds).EXPLAIN QUERY PLAN SELECT * FROM orders WHERE order_date = '2024-07-10'; will show SCAN TABLE orders.The correct index to create is on the order_date column:SQLCREATE INDEX idx_orders_order_date ON orders (order_date);
Execution will now be very fast (milliseconds).EXPLAIN QUERY PLAN SELECT * FROM orders WHERE order_date = '2024-07-10'; will now show SEARCH TABLE orders USING INDEX idx_orders_order_date (...).Chapter 12 (Level 8): Real-World Data ModelingData modeling is the capstone skill that synthesizes everything learned so far. It is the process of translating real-world business requirements into a logical, normalized, and efficient database schema. This chapter moves beyond using pre-existing tables and focuses on the design process itself.12.1 From Requirements to Schema: A Practical WalkthroughA robust data model is not created in a vacuum; it is the result of a structured process that begins with understanding the problem domain.60Gather Requirements: The first step is to understand the purpose of the database. This involves interviewing the people who will use it, analyzing existing business forms (invoices, reports), and defining the key questions the business needs to answer with this data.60 For example: "We need to track our customers and the orders they place."Identify Entities and Attributes: From the requirements, identify the main "nouns" or concepts. These will become your tables. Common examples are Customers, Products, Orders, Patients, and Doctors.72 For each entity, list its properties or attributes, which will become the columns (e.g., for a Customer: first_name, last_name, email).Identify Relationships and Cardinality: Determine how the entities relate to one another. Use verbs to describe the relationships (e.g., "a Customer places an Order"). Define the cardinality of the relationship 72:One-to-One (1:1): One customer has one profile.One-to-Many (1:M): One author writes many posts.Many-to-Many (M:N): One student enrolls in many courses, and one course has many students.Create a Logical Model (ERD): Draw an Entity-Relationship Diagram (ERD). This is a visual representation of the tables (as boxes), their attributes, and the lines connecting them to show relationships. This step helps clarify the design before writing any code.Create the Physical Model (CREATE TABLE): Translate the logical model into concrete CREATE TABLE statements. Choose appropriate data types, define NOT NULL and UNIQUE constraints, and establish the PRIMARY KEY and FOREIGN KEY relationships.Normalize and Refine: Apply the principles of normalization (1NF, 2NF, 3NF) to ensure the schema is free from anomalies and redundancy. Review the model for any exceptions or future needs.7312.2 Modeling Exercise 1: An E-commerce StoreRequirements: Design a database for an online store to track customers, products, and the orders they place. A customer can place multiple orders over time. A single order can contain multiple different products, and a product can appear in many different orders.Walkthrough:Entities: Customers, Products, Orders.Relationships:Customers to Orders is one-to-many (one customer can have many orders).Orders to Products is many-to-many. This is a key insight. A direct link is not possible. This requires a linking table (also called a junction or associative table) to resolve the M:N relationship. We will call this table Order_Items.Final Schema:SQLCREATE TABLE customers (
  customer_id INTEGER PRIMARY KEY,
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  email TEXT NOT NULL UNIQUE
);

CREATE TABLE products (
  product_id INTEGER PRIMARY KEY,
  name TEXT NOT NULL,
  category TEXT,
  price REAL NOT NULL CHECK (price >= 0)
);

CREATE TABLE orders (
  order_id INTEGER PRIMARY KEY,
  customer_id INTEGER NOT NULL,
  order_date TEXT NOT NULL,
  status TEXT DEFAULT 'Pending',
  FOREIGN KEY (customer_id) REFERENCES customers (customer_id)
);

CREATE TABLE order_items (
  order_id INTEGER NOT NULL,
  product_id INTEGER NOT NULL,
  quantity INTEGER NOT NULL CHECK (quantity > 0),
  price_per_unit REAL NOT NULL, -- Price at time of sale
  PRIMARY KEY (order_id, product_id),
  FOREIGN KEY (order_id) REFERENCES orders (order_id),
  FOREIGN KEY (product_id) REFERENCES products (product_id)
);
12.3 Modeling Exercise 2: A University SystemRequirements: Design a database for a university to manage students, professors, and courses. A professor can teach multiple courses, but each course is taught by only one professor. A student can enroll in many courses, and each course can have many students.Walkthrough:Entities: Students, Professors, Courses.Relationships:Professors to Courses is one-to-many.Students to Courses is many-to-many, which again requires a linking table. We will call this Enrollments.Final Schema:SQLCREATE TABLE students (
  student_id INTEGER PRIMARY KEY,
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  major TEXT
);

CREATE TABLE professors (
  professor_id INTEGER PRIMARY KEY,
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  department TEXT NOT NULL
);

CREATE TABLE courses (
  course_id INTEGER PRIMARY KEY,
  course_name TEXT NOT NULL UNIQUE,
  professor_id INTEGER,
  credits INTEGER CHECK (credits > 0),
  FOREIGN KEY (professor_id) REFERENCES professors (professor_id)
);

CREATE TABLE enrollments (
  student_id INTEGER NOT NULL,
  course_id INTEGER NOT NULL,
  grade TEXT,
  PRIMARY KEY (student_id, course_id),
  FOREIGN KEY (student_id) REFERENCES students (student_id),
  FOREIGN KEY (course_id) REFERENCES courses (course_id)
);
12.4 Modeling Exercise 3: A Healthcare ClinicRequirements: Design a database for a small clinic to track patients, doctors, and appointments. A patient can have many appointments. A doctor can have many appointments. Each appointment involves exactly one patient and one doctor.Walkthrough:Entities: Patients, Doctors, Appointments.Relationships: The Appointments entity itself acts as the link between patients and doctors. The relationship from Patients to Appointments is one-to-many. The relationship from Doctors to Appointments is also one-to-many.Final Schema:SQLCREATE TABLE patients (
  patient_id INTEGER PRIMARY KEY,
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  date_of_birth TEXT NOT NULL
);

CREATE TABLE doctors (
  doctor_id INTEGER PRIMARY KEY,
  first_name TEXT NOT NULL,
  last_name TEXT NOT NULL,
  specialty TEXT NOT NULL
);

CREATE TABLE appointments (
  appointment_id INTEGER PRIMARY KEY,
  patient_id INTEGER NOT NULL,
  doctor_id INTEGER NOT NULL,
  appointment_date TEXT NOT NULL,
  appointment_time TEXT NOT NULL,
  reason_for_visit TEXT,
  diagnosis TEXT,
  FOREIGN KEY (patient_id) REFERENCES patients (patient_id),
  FOREIGN KEY (doctor_id) REFERENCES doctors (doctor_id)
);
Part IV: Next Steps and Advanced TopicsChapter 13: Beyond SQLite: Migrating to Production SystemsYou have now mastered the fundamentals of SQL and data management using the powerful and convenient combination of SQLite and DBeaver. The skills you've acquired are universal. This final section provides a roadmap for applying these skills to larger, production-grade database systems like PostgreSQL and Snowflake, and introduces the concepts you'll need to bridge the gap.13.1 From File-Based to Server-Based: SQLite vs. PostgreSQL/SnowflakeSQLite is an exceptional tool for local development, testing, and embedded applications. However, most large-scale, multi-user applications rely on client-server database systems. Understanding when and why to graduate from SQLite is a key architectural decision.PostgreSQL is a powerful, open-source, enterprise-grade Object-Relational Database Management System (ORDBMS). It runs as a dedicated server process, handles high levels of concurrent writes, and provides a rich set of advanced features like stored procedures, complex data types, and robust user management. It is a common choice for the primary backend database for web applications.Snowflake is a modern, cloud-native data platform. It is designed for Online Analytical Processing (OLAP), data warehousing, and large-scale data analytics. Its architecture separates storage and compute, allowing for massive scalability and performance on analytical queries.The following table summarizes the key differences and primary use cases for these systems.Table: Comparison of Data SystemsFeatureSQLitePostgreSQLSnowflakeArchitectureEmbedded, Serverless (File-based)Client-Server (Monolithic)Cloud-Native (Separated Storage/Compute)Primary Use CaseLocal/Embedded Apps, Prototyping, TestingOLTP, Web Application Backend, General PurposeOLAP, Data Warehousing, Big Data AnalyticsConcurrencyLimited (one writer at a time in default mode)High (many concurrent readers and writers)Very High (multi-cluster compute)ScalabilityVertical (limited to a single machine)Vertical and Horizontal (Read Replicas)Horizontal (Elastic Compute Clusters)Setup & AdminZero/MinimalModerate to HighLow (Managed Service)CostFree (Open Source)Free (Open Source), plus hosting costsUsage-based (Pay for storage and compute time)13.2 Connecting DBeaver to PostgreSQL and SnowflakeA major benefit of learning with DBeaver is that your skills are immediately transferable. Connecting to a new database system is a familiar process.Connecting to PostgreSQL:In DBeaver, open the New Connection wizard (Database > New Database Connection).Search for and select "PostgreSQL". Click "Next".Unlike SQLite, you will not provide a file path. Instead, you will fill in server details 75:Host: The server address (e.g., localhost if running locally, or a cloud provider's URL).Port: The port PostgreSQL is running on (default is 5432).Database: The name of the specific database you want to connect to.Username: Your database username.Password: Your database password.Click "Test Connection...", download the driver if prompted, and then "Finish".Connecting to Snowflake:In the New Connection wizard, search for and select "Snowflake". Click "Next".Snowflake uses a slightly different set of parameters 8:Host: Your unique Snowflake account URL (e.g., youraccount.snowflakecomputing.com).Database: The name of the Snowflake database.Warehouse: The name of the virtual warehouse to use for running queries.Schema: The specific schema you want to work in.Authentication: Choose your authentication method (e.g., Username/Password, Private Key).Fill in your credentials, test the connection, and finish.Once connected, you will find that the DBeaver interface—the SQL editor, the database navigator, and the data viewers—works in exactly the same way as it did for SQLite.13.3 A Primer on Data Migration StrategiesMigrating data from one database system to another is a common task. The general process involves extracting the schema and data from the source, transforming it if necessary to match the target system's data types and syntax, and loading it into the target.Migrating from SQLite to PostgreSQL: While you can manually export to CSV and import, specialized tools make this much easier. pgloader is a powerful open-source command-line tool that can connect directly to an SQLite database file, automatically discover the schema, convert data types, and load the data into a PostgreSQL database, often with a single command.77Bashpgloader sqlite:///path/to/your/database.db postgresql://user:pass@host/dbname
Migrating to Snowflake: Migrating data to a cloud data warehouse like Snowflake typically involves an Extract, Load, Transform (ELT) approach. Data is often first extracted from the source and loaded into a staging area in the cloud (e.g., as CSV or Parquet files in an Amazon S3 bucket). From there, Snowflake's powerful COPY INTO command is used to efficiently load the data from the staging area into Snowflake tables.79 Tools like Snowflake's SnowConvert can help automate the initial code and schema conversion from other systems.81Chapter 14: What to Learn NextYou now have a solid and comprehensive foundation in SQL and data management. The following topics are logical next steps in your learning journey, building directly on the concepts covered in this course. Most of these are features of larger database systems like PostgreSQL.14.1 Simplifying Queries with VIEWsA VIEW is a virtual table based on the result-set of a stored SQL statement. It contains rows and columns, just like a real table, but it does not store the data itself. The data is generated dynamically whenever the view is queried.82Use Cases for Views:Simplifying Complexity: You can encapsulate a complex, multi-table join into a single view. Users can then query this view as if it were a simple table, without needing to understand the underlying joins.Security: You can create a view that exposes only certain columns or rows from a table, providing a secure way to grant users access to a subset of data without giving them access to the entire table.Logical Data Independence: A view can provide a stable, consistent interface to users, even if the underlying table structures change.Syntax:SQLCREATE VIEW Customer_Order_Summary AS
SELECT
  c.customer_id,
  c.email,
  COUNT(o.order_id) AS number_of_orders,
  SUM(o.total_amount) AS total_spent
FROM customers AS c
LEFT JOIN orders AS o ON c.customer_id = o.customer_id
GROUP BY c.customer_id, c.email;

-- Now you can query the view like a table
SELECT * FROM Customer_Order_Summary WHERE number_of_orders > 5;
14.2 Ensuring Data Integrity with TRANSACTIONsA transaction is a sequence of one or more SQL operations that are executed as a single, indivisible unit of work.85 Transactions are the cornerstone of data integrity in relational databases.The reliability of transactions is guaranteed by the ACID properties 85:Atomicity: All operations within the transaction either succeed completely, or they all fail and are undone. The transaction is an "all or nothing" proposition.Consistency: The transaction is guaranteed to bring the database from one valid state to another. It will not violate integrity constraints.Isolation: Transactions running concurrently are isolated from each other, preventing them from interfering with one another's intermediate, uncommitted data.Durability: Once a transaction has been successfully completed (committed), its changes are permanent and will survive any subsequent system failure.Core Commands:BEGIN TRANSACTION; (or START TRANSACTION;): Marks the beginning of a transaction.COMMIT;: Makes all changes since the BEGIN statement permanent.ROLLBACK;: Undoes all changes made since the BEGIN statement, reverting the database to its state before the transaction started.The classic example is a bank transfer:SQLBEGIN TRANSACTION;

-- Deduct $100 from Account A
UPDATE accounts SET balance = balance - 100 WHERE account_id = 'A';

-- Add $100 to Account B
UPDATE accounts SET balance = balance + 100 WHERE account_id = 'B';

-- If both operations succeed, make them permanent
COMMIT;

-- If an error occurred, a ROLLBACK would be issued to undo both changes.
14.3 Encapsulating Logic with Stored Procedures (Conceptual)A stored procedure is a set of SQL statements that is stored in the database and can be executed simply by calling its name.88Important Note: SQLite does not support stored procedures. This is a key feature of more advanced client-server databases like PostgreSQL, SQL Server, and Oracle.Benefits of Stored Procedures:Reusability and Centralization: Complex business logic can be written once, stored in the database, and called by multiple applications, ensuring consistency.Performance: Executing a single call to a stored procedure can be more efficient than sending multiple individual SQL statements over the network, as it reduces network traffic.Security: You can grant a user permission to execute a stored procedure without granting them direct access to the underlying tables.While you cannot create them in SQLite, it is important to be aware of this concept as you progress to other database systems.Appendix: Sample DatasetsThe following datasets are provided in CSV format to support the mini-projects and exercises throughout this course. They have been curated and cleaned from public sources, primarily Kaggle, to provide realistic and well-structured data for learning.91A.1 E-commerce Storecustomers.csv:customer_id (INTEGER, Primary Key)first_name (TEXT)last_name (TEXT)email (TEXT)registration_date (TEXT)products.csv:product_id (INTEGER, Primary Key)name (TEXT)category (TEXT)brand (TEXT)price (REAL)orders.csv:order_id (INTEGER, Primary Key)customer_id (INTEGER, Foreign Key to customers)order_date (TEXT)status (TEXT)order_items.csv:order_item_id (INTEGER, Primary Key)order_id (INTEGER, Foreign Key to orders)product_id (INTEGER, Foreign Key to products)quantity (INTEGER)price (REAL)A.2 University Systemstudents.csv:student_id (INTEGER, Primary Key)first_name (TEXT)last_name (TEXT)major (TEXT)enrollment_year (INTEGER)professors.csv:professor_id (INTEGER, Primary Key)first_name (TEXT)last_name (TEXT)department (TEXT)courses.csv:course_id (INTEGER, Primary Key)course_name (TEXT)professor_id (INTEGER, Foreign Key to professors)credits (INTEGER)enrollments.csv:enrollment_id (INTEGER, Primary Key)student_id (INTEGER, Foreign Key to students)course_id (INTEGER, Foreign Key to courses)grade (TEXT)A.3 Healthcare Clinicpatients.csv:patient_id (INTEGER, Primary Key)first_name (TEXT)last_name (TEXT)date_of_birth (TEXT)gender (TEXT)doctors.csv:doctor_id (INTEGER, Primary Key)first_name (TEXT)last_name (TEXT)specialty (TEXT)visits.csv:visit_id (INTEGER, Primary Key)patient_id (INTEGER, Foreign Key to patients)doctor_id (INTEGER, Foreign Key to doctors)visit_date (TEXT)diagnosis (TEXT)medications.csv:medication_id (INTEGER, Primary Key)visit_id (INTEGER, Foreign Key to visits)medication_name (TEXT)dosage (TEXT)A.4 Data Generation ScriptsFor learners who wish to experiment with larger datasets for performance testing, a set of Python scripts using the Faker library is provided. These scripts can be used to generate millions of rows of synthetic but realistic data for the schemas defined above.